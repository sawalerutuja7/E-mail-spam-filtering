# -*- coding: utf-8 -*-
"""E-mail spam filtering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qhhUpbDwpGiIiyHz5I-DnLrwI0awS48f
"""

from nltk.tokenize import RegexpTokenizer, word_tokenize
from nltk.stem import PorterStemmer
from nltk.corpus import stopwords
import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split

df = pd.read_csv('spam.csv', encoding='ISO-8859-1')
le = LabelEncoder()
data = df.to_numpy()
X = data[:, 1]
y = data[:, 0]
X.shape, y.shape

df.head()

df.shape

df['Spam'] = df['v1'].apply(lambda x:1 if x=='spam' else 0)
df.head(5)

import string
import re

# NLP modules
import spacy
import nltk

from spacy.lang.en.examples import sentences
nlp = spacy.load('en_core_web_sm')
print(type(nlp))

from nltk.corpus import stopwords

# Save stopwords as a set
stopWords = set(stopwords.words('english'))
type(stopWords)
print(stopWords)

punctuations = string.punctuation
print(punctuations)

# `Function message_cleaner
def message_cleaner(sentence):
    """
    Function message_cleaner clean the text using typical Natural Language Processing
    (NLP) steps.
    Steps include: Lemmatization, removing stop words, removing punctuations
    Args:
        sentence (str): The uncleaned text.
    Returns:
        str: The cleaned text.
    """
    # Create the Doc object named `text` from `sentence` using `nlp()`
    text = nlp(sentence)
    # Lemmatization - remove the lemmas -PRON-
    text = [ token.lemma_ for token in text if token.lemma_ != "-PRON-"]
    # Remove stop words
    text = [ token for token in text if token not in stopWords ]
    # Remove punctuations
    text = [ token for token in text if token not in punctuations]
    # Use the .join() method on text to convert string
    text = " ".join(text)
    # Use re.sub() to substitute multiple spaces or dots`[\.\s]+` to single space `' '`
    text  = re.sub('[\.\s]+', ' ', text)

    # Return the cleaned text
    return text

"""df.loc[:,'message_cleaned'] = df.loc[:,'Message'].apply(message_cleaner)"""

df

from sklearn.model_selection import train_test_split

# Define feature and target
features = df.loc[:, 'Unnamed: 2']
target = df.loc[:, 'Spam']

# Determine train and test feature and target
# 30% of features are thus assigned to the test data set
# Set the value 1 for random_state (to make our results reproducible)
features_train, features_test, target_train, target_test = train_test_split(features,
                                                                            target,
                                                                            test_size = 0.3,
                                                                            random_state = 1)



import nltk
nltk.download('stopwords')
tokenizer = RegexpTokenizer('\w+')
sw = set(stopwords.words('english'))
ps = PorterStemmer()

def getStem(review):
    review = review.lower()
    tokens = tokenizer.tokenize(review) # breaking into small words
    removed_stopwords = [w for w in tokens if w not in sw]
    stemmed_words = [ps.stem(token) for token in removed_stopwords]
    clean_review = ' '.join(stemmed_words)
    return clean_review

def getDoc(document):
    d = []
    for doc in document:
        d.append(getStem(doc))
    return d

stemmed_doc = getDoc(X)

stemmed_doc[:10]

cv = CountVectorizer()
vc = cv.fit_transform(stemmed_doc)
X = vc.todense()
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

df

messages = [
    """Join us today at 12:00 PM ET / 16:00 UTC for a Red Hat DevNation tech talk on AWS Lambda and serverless Java with Bill Burke.
Have you ever tried Java on AWS Lambda but found that the cold-start latency and memory usage were far too high?
In this session, we will show how we optimized Java for serverless applications by leveraging GraalVM with Quarkus to
provide both supersonic startup speed and a subatomic memory footprint.""",

    """You have space available
According to our records, you have space available in your Dropbox! Make the most of your account nowâ€”you'll be able to access your files from anywhere.
Continue
Not sure where to start? Check out our Fundamentals course to learn the essentials of your account."""
]

def prepare(messages):
    d = getDoc(messages)
    # dont do fit_transform!! it will create new vocab.
    return cv.transform(d)

messages = prepare(messages)

